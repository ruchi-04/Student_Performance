# -*- coding: utf-8 -*-
"""Copy of Student_performance_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/148AtZhep7L2k1OrnFfPpOmkqImX-yxgx
"""

# import necessary libraries
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# loading the dataset from excel file to pandas DataFrame
df=pd.read_csv("/content/student-por.csv")

# first 5 rows of the dataframe
df.head()

# number of data points & number of features
df.shape

print('Describing the Dataset:')
print(df.describe())

df.info()

# checking for missing values
df.isna().sum()

#need name of categorical columns
categorical_cols = df.select_dtypes(include='object').columns
print(categorical_cols)

#shows unique feature of our column
for i in categorical_cols:
  print(df[i].unique())

#convert mixed data to string
for i in [4]:
  df= df.astype(str)

df.head()

#LabelEncoding to convert categorical data to numerical data
from sklearn.preprocessing import LabelEncoder
for i in ['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']:
  df[i]=LabelEncoder().fit_transform(df[i])
print(df.head())

#from matplotlib.axis import YAxis
X=df.drop(columns=['G3'])
Y=df['G3']
xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=0.2)
print(xtrain.shape,xtest.shape)

from sklearn.preprocessing import MinMaxScaler #Normalisation because dataset is small
print(MinMaxScaler().fit_transform(df))

#decisionTree
model_dt=DecisionTreeClassifier(criterion="entropy",max_depth=3)
model_dt.fit(xtrain,ytrain)
pred_train=model_dt.predict(xtrain)
pred_test=model_dt.predict(xtest)
print("training accuracy is ",accuracy_score(ytrain,pred_train))
print("testing accuracy is ",accuracy_score(ytest,pred_test))

#SVM
from sklearn.svm import SVC
model_2=SVC(kernel='linear')
model_2.fit(xtrain,ytrain)
pred_train=model_2.predict(xtrain)
pred_test=model_2.predict(xtest)
print("training accuracy is ",accuracy_score(ytrain,pred_train))
print("testing accuracy is ",accuracy_score(ytest,pred_test))

#Perceptron
from sklearn.linear_model import Perceptron
model_3=Perceptron()
model_3.fit(xtrain,ytrain)
pred_train=model_3.predict(xtrain)
pred_test=model_3.predict(xtest)
print("training accuracy is ",accuracy_score(ytrain,pred_train))
print("testing accuracy is ",accuracy_score(ytest,pred_test))

#knn
from sklearn.neighbors import KNeighborsClassifier
model_4=KNeighborsClassifier()
model_4.fit(xtrain,ytrain)
pred_train=model_4.predict(xtrain)
pred_test=model_4.predict(xtest)
print("training accuracy is ",accuracy_score(ytrain,pred_train))
print("testing accuracy is ",accuracy_score(ytest,pred_test))

#logistic Regression
# from sklearn.linear_model import LogisticRegression
# model_3=LogisticRegression(max_iter=10000)
# model_3.fit(xtrain,ytrain)
# pred_train=model_3.predict(xtrain)
# pred_test=model_3.predict(xtest)
# print("training accuracy is ",accuracy_score(ytrain,pred_train))
# print("testing accuracy is ",accuracy_score(ytest,pred_test))

#GaussiaNB
from sklearn.naive_bayes import GaussianNB
model_5=GaussianNB()
model_5.fit(xtrain,ytrain)
pred_train=model_5.predict(xtrain)
pred_test=model_5.predict(xtest)
print("training accuracy is ",accuracy_score(ytrain,pred_train))
print("testing accuracy is ",accuracy_score(ytest,pred_test))

#Hyperparameter Tuning
from sklearn.model_selection import GridSearchCV

# create a dictionary of hyperparameters for each model
params_lr = {'C': [0.01, 0.1, 1]}
params_sv = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4]}
params_dt = {'max_depth': [2, 4, 6, 8], 'min_samples_split': [2, 4, 6, 8]}
params_p = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1]}
param_g = {'n_neighbors': [3, 5, 7, 9],'weights': ['uniform', 'distance']}

# create the models
#lr = LogisticRegression(max_iter=10000)
sv = SVC()
dt = DecisionTreeClassifier()
p = Perceptron()
knn = KNeighborsClassifier()

# create the GridSearchCV objects
#grid_lr = GridSearchCV(lr, params_lr, cv=5)
grid_sv = GridSearchCV(sv, params_sv, cv=5)
grid_dt = GridSearchCV(dt, params_dt, cv=5)
grid_p = GridSearchCV(p, params_p, cv=5)
grid_s = GridSearchCV(knn, param_g, cv=5)

# fit the models to the training data
#grid_lr.fit(xtrain, ytrain)
grid_sv.fit(xtrain, ytrain)
grid_dt.fit(xtrain, ytrain)
grid_p.fit(xtrain, ytrain)
grid_s.fit(xtrain, ytrain)


# print the best hyperparameters and test accuracy for each model
# print("Logistic Regression: best hyperparameters: ", grid_lr.best_params_)
# print("test accuracy: ", grid_lr.score(xtest, ytest))

print("SVM: best hyperparameters: ", grid_sv.best_params_)
print("test accuracy: ", grid_sv.score(xtest, ytest))

print("Decision Tree: best hyperparameters: ", grid_dt.best_params_)
print("test accuracy: ", grid_dt.score(xtest, ytest))

print("Perceptron: best hyperparameters: ", grid_p.best_params_)
print("test accuracy: ", grid_p.score(xtest, ytest))

print('k nearesr neighbours: Best hyperparameters:', grid_s.best_params_)
print('Best score:', grid_s.best_score_)